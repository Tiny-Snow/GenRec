# SASRec2HSTU-Spring + BCE on Amazons

# global settings
pretrained_ckpt: null  # optional path to a pretrained checkpoint to load
test_eval: false  # whether to run evaluation on the test set instead of validation set
save_predictions: false  # whether to save the predictions on the test set

# dataset settings
dataset:
    type: seqrec

    search__interaction_data_path:
    - /home/ywq/GenRec/data/amazon2014-beauty/proc/user2item.pkl  # Amazon-2014-Beauty
    
    max_seq_length: 50

# collator settings
collator:
    type: seqrec

    num_negative_samples: 16
    negative_sampling_strategy: uniform

# model settings
model:
    type: sasrec2hstu_spring

    config:
        # base model parameters
        hidden_size: 256
        search__num_attention_heads: [1]
        search__num_hidden_layers: [2, 4]

        # subclass model parameters
        max_seq_len: 50
        search__learnable_input_pos_emb: [false]
        search__linear_dropout: [0.0]
        search__attention_type: ["softmax", "silu", "silu_norm"]

        search__spring_attention_weight: [5.0, 1.0, 0.1, 0]
        search__spring_ffn_weight: [0]
        search__spring_emb_weight: [0]
        search__spring_attention_temperature: [2.0, 4.0]
        spectral_norm_iters: 1

# trainer settings
trainer:
    type: bce

    config:
        # training arguments - Run control
        do_train: true
        do_eval: true
        do_predict: true

        # training arguments - Optimization & schedule
        num_train_epochs: 200
        per_device_train_batch_size: 512
        per_device_eval_batch_size: 1024
        gradient_accumulation_steps: 1  # batch_size = per_device_train_batch_size * num_devices * gradient_accumulation_steps
        learning_rate: 1.0e-3
        search__weight_decay: [0.1]
        lr_scheduler_type: linear
        warmup_ratio: 0.05

        # training arguments - Evaluation & checkpointing
        metric_for_best_model: ndcg@5  # should exist in the metrics

        # training arguments - Parallelism & precision
        dataloader_num_workers: 4

        # base trainer parameters
        norm_embeddings: false  # whether to L2-normalize user and item embeddings
        eval_interval: 5  # run metrics every epoch
        metrics:
        - ["hr", {}]
        - ["ndcg", {}]
        - ["popularity", {p: [0.1, 0.2]}]
        - ["unpopularity", {p: [0.2, 0.4]}]
        model_loss_weight: 1.0
        top_k: [1, 5, 10]

        # subclass trainer parameters


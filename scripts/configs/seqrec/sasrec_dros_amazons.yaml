# SASRec + DROS on Amazon datasets

# global settings
pretrained_ckpt: null  # optional path to a pretrained checkpoint to load
test_eval: false  # whether to run evaluation on the test set instead of validation set
save_predictions: false  # whether to save the predictions on the test set

# dataset settings
dataset:
    type: seqrec

    search__interaction_data_path:
    - /home/ywq/GenRec/data/amazon2014-beauty/proc/user2item.pkl  # Amazon-2014-Beauty
    - /home/ywq/GenRec/data/amazon2014-book_1000000/proc/user2item.pkl  # Amazon-2014-Book-1M
    # - /home/ywq/GenRec/data/amazon2014-cd/proc/user2item.pkl  # Amazon-2014-CD
    # - /home/ywq/GenRec/data/amazon2014-clothing/proc/user2item.pkl  # Amazon-2014-Clothing
    # - /home/ywq/GenRec/data/amazon2014-electronic_1000000/proc/user2item.pkl  # Amazon-2014-Electronic-1M
    max_seq_length: 50

# collator settings
collator:
    type: seqrec

    num_negative_samples: 16
    negative_sampling_strategy: uniform

# model settings
model:
    type: sasrec

    config:
        # base model parameters
        hidden_size: 256
        search__num_attention_heads: [1, 8]
        search__num_hidden_layers: [2, 4, 6, 8]

        # subclass model parameters
        search__attention_dropout: [0.2, 0.4]

# trainer settings
trainer:
    type: dros

    config:
        # training arguments - Run control
        do_train: true
        do_eval: true
        do_predict: true

        # training arguments - Optimization & schedule
        num_train_epochs: 200
        per_device_train_batch_size: 512
        per_device_eval_batch_size: 1024
        gradient_accumulation_steps: 1  # batch_size = per_device_train_batch_size * num_devices * gradient_accumulation_steps
        learning_rate: 1.0e-3
        search__weight_decay: [0.1]
        lr_scheduler_type: linear
        warmup_ratio: 0.05

        # training arguments - Evaluation & checkpointing
        metric_for_best_model: ndcg@5  # should exist in the metrics

        # training arguments - Parallelism & precision
        dataloader_num_workers: 4

        # base trainer parameters
        eval_interval: 5  # run metrics every epoch
        metrics:
        - ["hr", {}]
        - ["ndcg", {}]
        - ["popularity", {p: [0.1, 0.2]}]
        - ["unpopularity", {p: [0.2, 0.4]}]
        model_loss_weight: 0.0
        top_k: [1, 5, 10]

        # subclass trainer parameters
        search__dros_temperature: [0.5, 1.0, 1.5]
        search__dros_weight: [0.1, 0.5]
        popularity_temperature: 0.05


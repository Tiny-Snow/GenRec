# An example search config for HSTU-Spring + BCE loss on Movielens-1M dataset with grid search
# Here `search__{hp}` indicates that `{hp}` is a list of hyperparameters to search over.

# global settings
pretrained_ckpt: null  # optional path to a pretrained checkpoint to load
test_eval: false  # whether to run evaluation on the test set instead of validation set
save_predictions: false  # whether to save the predictions on the test set

# dataset settings
dataset:
    type: seqrec

    # specific path to interaction data file for Movielens-1M dataset.
    # You may add new datasets here to run experiments in parallel.
    search__interaction_data_path:
    - /path/to/movielens-1m/proc/user2item.pkl
    max_seq_length: 200

# collator settings
collator:
    type: seqrec

    num_negative_samples: 32
    negative_sampling_strategy: uniform

# model settings
model:
    type: hstu_spring

    config:
        # base model parameters
        hidden_size: 256
        search__num_attention_heads: [1, 2, 4, 8]
        search__num_hidden_layers: [2, 4, 6, 8]

        # subclass model parameters
        search__linear_dropout: [0, 0.2, 0.4]
        search__attention_dropout: [0, 0.2, 0.4]
        max_seq_len: 200
        num_buckets: 128
        enable_input_pos_emb: false  # disable absolute input positional embeddings
        enable_learnable_rel_posemb: false  # use RoPE instead of learnable relative positional embeddings
        enable_attention_gating: false  # disable attention gating mechanism
        enable_ffn: true  # enable feed-forward network
        enable_final_layernorm: true  # enable final layer normalization

        search__spring_attention_weight: [5.0, 2.5, 1.0, 1.0e-1]
        search__spring_ffn_weight: [0, 1.0e-3, 1.0e-4]
        search__spring_emb_weight: [0, 0.1, 0.01]
        search__spring_attention_temperature: [1.0, 2.0, 4.0, 8.0]
        spectral_norm_iters: 1
        norm_embeddings: false  # whether to L2-normalize item embeddings when calculating spring regularization

# trainer settings
trainer:
    type: bce

    config:
        # training arguments - Run control
        do_train: true
        do_eval: true
        do_predict: true

        # training arguments - Optimization & schedule
        num_train_epochs: 200
        per_device_train_batch_size: 128
        per_device_eval_batch_size: 256
        gradient_accumulation_steps: 1  # batch_size = per_device_train_batch_size * num_devices * gradient_accumulation_steps
        learning_rate: 1.0e-3
        search__weight_decay: [0, 0.1]
        lr_scheduler_type: linear
        warmup_ratio: 0.05

        # training arguments - Evaluation & checkpointing
        metric_for_best_model: ndcg@5  # should exist in the metrics

        # training arguments - Parallelism & precision
        dataloader_num_workers: 4
        gradient_checkpointing: false
        bf16: false
        tf32: true

        # base trainer parameters
        norm_embeddings: false  # whether to L2-normalize user and item embeddings
        eval_interval: 5  # run metrics every epoch
        metrics:
        - ["hr", {}]
        - ["ndcg", {}]
        - ["popularity", {p: [0.1, 0.2]}]
        - ["unpopularity", {p: [0.2, 0.4]}]
        model_loss_weight: 1.0
        top_k: [1, 5, 10]

